{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3536c2ff-8aff-4b43-b894-2b884ed6ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Collecting Inputs ---\n",
      "Brand URL: https://www.allbirds.com\n",
      "Competitor URL: https://www.rothys.com\n",
      "Service Locations: New York, NY, Los Angeles, CA, London, UK, Berlin, Germany, Sydney, Australia\n",
      "\n",
      "--- Step 2: Scraping Websites for Keyword Discovery ---\n",
      "Scraping complete. Generating initial keyword ideas using Gemini API...\n",
      "\n",
      "--- Step 3: Simulating Keyword Planner Data ---\n",
      "Total keywords found (before filtering): 20\n",
      "\n",
      "--- Step 4: Filtering Keywords (Search Volume > 500) ---\n",
      "Keywords after filtering: 19\n",
      "\n",
      "--- Step 5: Grouping Keywords into Ad Groups ---\n",
      "Deliverable successfully generated and saved to 'sem_deliverable_1_output.txt'\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "# --- Gemini API Configuration (Leave API key empty, Canvas will provide) ---\n",
    "API_KEY = \"AIzaSyCKNKW9HKVwVjQRDeT0lbSUz8Jh-FIeE9M\"\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent\"\n",
    "\n",
    "# --- LLM Simulation Functions ---\n",
    "async def llm_generate_keywords(brand_content, competitor_content, locations):\n",
    "    \"\"\"\n",
    "    Generates keywords using the Gemini API based on website content.\n",
    "    Includes exponential backoff for API calls.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following content from a brand's website and its competitor.\n",
    "    Identify 10-15 highly relevant, high-intent seed keywords that a potential customer\n",
    "    would use to search for these products/services. Include brand terms, competitor terms,\n",
    "    and general category terms. Also, consider adding location-specific keywords for these areas: {', '.join(locations)}.\n",
    "    Provide the keywords as a comma-separated list.\n",
    "\n",
    "    Brand Content (from {brand_content[:100]}...):\n",
    "    {brand_content[:1000]}\n",
    "\n",
    "    Competitor Content (from {competitor_content[:100]}...):\n",
    "    {competitor_content[:1000]}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_history = []\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n",
    "    payload = {\"contents\": chat_history}\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    full_api_url = f\"{API_URL}?key={API_KEY}\"\n",
    "\n",
    "    retries = 0\n",
    "    max_retries = 5\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(full_api_url, headers=headers, data=json.dumps(payload))\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "\n",
    "            if result.get(\"candidates\") and result[\"candidates\"][0].get(\"content\") and result[\"candidates\"][0][\"content\"].get(\"parts\"):\n",
    "                text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "                keywords = [kw.strip() for kw in text.split(',') if kw.strip()]\n",
    "                return keywords\n",
    "            else:\n",
    "                print(f\"LLM response structure unexpected: {result}\")\n",
    "                time.sleep(2 ** retries)\n",
    "                retries += 1\n",
    "                continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API call failed (retry {retries+1}/{max_retries}): {e}\")\n",
    "            time.sleep(2 ** retries)\n",
    "            retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            time.sleep(2 ** retries)\n",
    "            retries += 1\n",
    "\n",
    "    print(\"Failed to generate keywords after multiple retries.\")\n",
    "    return [\n",
    "        \"allbirds shoes\", \"rothys shoes\", \"sustainable sneakers\",\n",
    "        \"wool runners\", \"tree dashers\", \"best comfortable travel shoes\",\n",
    "        \"allbirds review\", \"rothys flats\", \"allbirds vs rothys\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def llm_group_keywords(keywords_data, brand_name, competitor_name):\n",
    "    \"\"\"\n",
    "    Simulates an LLM grouping keywords into ad groups based on intent.\n",
    "    \"\"\"\n",
    "    ad_groups = {\n",
    "        \"Brand Terms\": [],\n",
    "        \"Product/Service Category\": [],\n",
    "        \"Competitor Terms\": [],\n",
    "        \"Long-Tail / Informational\": [],\n",
    "        \"Location-Based Queries\": []\n",
    "    }\n",
    "\n",
    "    brand_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        brand_name.replace('.', '\\\\.?'), 'allbirds', 'all birds', 'wool runners', 'tree dashers'\n",
    "    ]) + r')\\b'\n",
    "    competitor_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        competitor_name.replace('.', '\\\\.?'), 'rothys', 'rothys shoes', 'reputation.com'\n",
    "    ]) + r')\\b'\n",
    "    \n",
    "    brand_pattern = re.compile(brand_keywords_regex, re.IGNORECASE)\n",
    "    competitor_pattern = re.compile(competitor_keywords_regex, re.IGNORECASE)\n",
    "\n",
    "    for item in keywords_data:\n",
    "        kw = item['keyword'].lower()\n",
    "        \n",
    "        is_brand_term = bool(brand_pattern.search(kw))\n",
    "        is_competitor_term = bool(competitor_pattern.search(kw))\n",
    "\n",
    "        if is_brand_term and not is_competitor_term:\n",
    "            ad_groups[\"Brand Terms\"].append(item)\n",
    "        elif is_competitor_term:\n",
    "            ad_groups[\"Competitor Terms\"].append(item)\n",
    "        elif \"shoes\" in kw or \"sneakers\" in kw or \"runners\" in kw or \"flats\" in kw or \\\n",
    "             \"marketing platform\" in kw or \"seo\" in kw or \"ads optimization\" in kw or \"reputation management\" in kw:\n",
    "            ad_groups[\"Product/Service Category\"].append(item)\n",
    "        elif \"new york\" in kw or \"los angeles\" in kw or \"london\" in kw or \"berlin\" in kw or \"sydney\" in kw or \\\n",
    "             \"san ramon\" in kw or \"chicago\" in kw or \"scottsdale\" in kw or \"lehi\" in kw or \\\n",
    "             \"liverpool\" in kw or \"munich\" in kw or \"mannheim\" in kw or \"hyderabad\" in kw:\n",
    "            ad_groups[\"Location-Based Queries\"].append(item)\n",
    "        else:\n",
    "            ad_groups[\"Long-Tail / Informational\"].append(item)\n",
    "            \n",
    "    for group in ad_groups:\n",
    "        for item in ad_groups[group]:\n",
    "            if group == \"Brand Terms\":\n",
    "                item['suggested_match_type'] = \"Exact\"\n",
    "            elif group == \"Competitor Terms\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            elif group == \"Product/Service Category\" or group == \"Location-Based Queries\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            else:\n",
    "                item['suggested_match_type'] = \"Broad\"\n",
    "\n",
    "    return ad_groups\n",
    "\n",
    "\n",
    "# --- Web Scraping Function ---\n",
    "def get_website_content(url):\n",
    "    \"\"\"Fetches and scrapes text content from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return \" \".join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- Keyword Planner Data Simulation ---\n",
    "def simulate_keyword_planner_data(keywords):\n",
    "    \"\"\"\n",
    "    Simulates fetching data from a keyword planner API alternative.\n",
    "    Generates random but realistic metrics.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for kw in keywords:\n",
    "        if len(kw.split()) < 3:\n",
    "            avg_monthly_searches = random.randint(1000, 100000)\n",
    "            low_bid = round(random.uniform(0.5, 3.0), 2)\n",
    "            high_bid = round(random.uniform(3.5, 10.0), 2)\n",
    "            competition = \"High\" if random.random() > 0.5 else \"Medium\"\n",
    "        else:\n",
    "            avg_monthly_searches = random.randint(50, 5000)\n",
    "            low_bid = round(random.uniform(0.2, 1.5), 2)\n",
    "            high_bid = round(random.uniform(1.8, 5.0), 2)\n",
    "            competition = \"Medium\" if random.random() > 0.3 else \"Low\"\n",
    "        \n",
    "        data.append({\n",
    "            \"keyword\": kw,\n",
    "            \"avg_monthly_searches\": avg_monthly_searches,\n",
    "            \"top_of_page_bid_low\": low_bid,\n",
    "            \"top_of_page_bid_high\": high_bid,\n",
    "            \"competition\": competition\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- Main Logic ---\n",
    "async def main():\n",
    "    # Load inputs from config.yaml\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    brand_url = config['brand_website']\n",
    "    competitor_url = config['competitor_website']\n",
    "    service_locations = config['service_locations']\n",
    "    brand_name = brand_url.replace('https://www.', '').split('.')[0]\n",
    "    competitor_name = competitor_url.replace('https://www.', '').split('.')[0]\n",
    "\n",
    "\n",
    "    print(\"--- Step 1: Collecting Inputs ---\")\n",
    "    print(f\"Brand URL: {brand_url}\")\n",
    "    print(f\"Competitor URL: {competitor_url}\")\n",
    "    print(f\"Service Locations: {', '.join(service_locations)}\\n\")\n",
    "\n",
    "    print(\"--- Step 2: Scraping Websites for Keyword Discovery ---\")\n",
    "    brand_content = get_website_content(brand_url)\n",
    "    competitor_content = get_website_content(competitor_url)\n",
    "    print(\"Scraping complete. Generating initial keyword ideas using Gemini API...\\n\")\n",
    "\n",
    "    # Use LLM to generate initial keywords\n",
    "    master_keyword_list = await llm_generate_keywords(brand_content, competitor_content, service_locations)\n",
    "    \n",
    "    print(\"--- Step 3: Simulating Keyword Planner Data ---\")\n",
    "    keyword_df = simulate_keyword_planner_data(master_keyword_list)\n",
    "    print(f\"Total keywords found (before filtering): {len(keyword_df)}\\n\")\n",
    "\n",
    "    print(\"--- Step 4: Filtering Keywords (Search Volume > 500) ---\")\n",
    "    filtered_df = keyword_df[keyword_df['avg_monthly_searches'] >= 500]\n",
    "    print(f\"Keywords after filtering: {len(filtered_df)}\\n\")\n",
    "\n",
    "    print(\"--- Step 5: Grouping Keywords into Ad Groups ---\")\n",
    "    final_keywords_dict = llm_group_keywords(filtered_df.to_dict('records'), brand_name, competitor_name)\n",
    "\n",
    "    output_filename = \"sem_deliverable_1_output.txt\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(f\"## Deliverable #1: Keyword List Grouped by Ad Groups ({brand_name})\\n\\n\")\n",
    "        f.write(\"Based on brand website content, competitor insights, and simulated keyword data with specific location targeting.\\n\\n\")\n",
    "        \n",
    "        for ad_group, keywords in final_keywords_dict.items():\n",
    "            if keywords:\n",
    "                f.write(f\"### Ad Group: {ad_group}\\n\")\n",
    "                f.write(\"--------------------------------\\n\")\n",
    "                \n",
    "                for kw_data in keywords:\n",
    "                    f.write(\n",
    "                        f\" - Keyword: {kw_data['keyword']}\\n\"\n",
    "                        f\"   - Suggested Match Type: {kw_data['suggested_match_type']}\\n\"\n",
    "                        f\"   - Suggested CPC Range: ${kw_data['top_of_page_bid_low']} - ${kw_data['top_of_page_bid_high']}\\n\"\n",
    "                        f\"   - Monthly Searches: {kw_data['avg_monthly_searches']}\\n\"\n",
    "                        f\"   - Competition: {kw_data['competition']}\\n\"\n",
    "                        f\"\\n\"\n",
    "                    )\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "    print(f\"Deliverable successfully generated and saved to '{output_filename}'\")\n",
    "\n",
    "# At the very end of the cell:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85501489-b751-419e-87d2-cc39ca8043d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Collecting Inputs ---\n",
      "Brand URL: https://www.allbirds.com\n",
      "Competitor URL: https://www.rothys.com\n",
      "Service Locations: New York, NY, Los Angeles, CA, London, UK, Berlin, Germany, Sydney, Australia\n",
      "Shopping Budget: $4000, Search Budget: $5000, PMax Budget: $2500\n",
      "\n",
      "--- Step 2: Scraping Websites for Keyword Discovery ---\n",
      "Scraping complete. Generating initial keyword ideas using Gemini API...\n",
      "\n",
      "--- Step 3: Simulating Keyword Planner Data ---\n",
      "Total keywords found (before filtering): 17\n",
      "\n",
      "--- Step 4: Filtering Keywords (Search Volume > 500) ---\n",
      "Keywords after filtering: 17\n",
      "\n",
      "--- Step 5: Grouping Keywords into Ad Groups ---\n",
      "All deliverables successfully generated and saved to 'sem_plan_full_output.txt'\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "# --- Gemini API Configuration (Leave API key empty, Canvas will provide) ---\n",
    "API_KEY = \"AIzaSyCKNKW9HKVwVjQRDeT0lbSUz8Jh-FIeE9M\" # If you want to use models other than gemini-2.5-flash-preview-05-20, provide an API key here. Otherwise, leave this as-is.\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent\"\n",
    "\n",
    "# --- LLM Simulation Functions ---\n",
    "async def llm_generate_keywords(brand_content, competitor_content, locations):\n",
    "    \"\"\"\n",
    "    Generates keywords using the Gemini API based on website content.\n",
    "    Includes exponential backoff for API calls.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following content from a brand's website and its competitor.\n",
    "    Identify 10-15 highly relevant, high-intent seed keywords that a potential customer\n",
    "    would use to search for these products/services. Include brand terms, competitor terms,\n",
    "    and general category terms. Also, consider adding location-specific keywords for these areas: {', '.join(locations)}.\n",
    "    Provide the keywords as a comma-separated list.\n",
    "\n",
    "    Brand Content (from {brand_content[:100]}...):\n",
    "    {brand_content[:1000]}\n",
    "\n",
    "    Competitor Content (from {competitor_content[:100]}...):\n",
    "    {competitor_content[:1000]}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_history = []\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n",
    "    payload = {\"contents\": chat_history}\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    full_api_url = f\"{API_URL}?key={API_KEY}\"\n",
    "\n",
    "    retries = 0\n",
    "    max_retries = 5\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(full_api_url, headers=headers, data=json.dumps(payload))\n",
    "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "            result = response.json()\n",
    "\n",
    "            if result.get(\"candidates\") and result[\"candidates\"][0].get(\"content\") and result[\"candidates\"][0][\"content\"].get(\"parts\"):\n",
    "                text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "                # Split the text by comma and clean up whitespace\n",
    "                keywords = [kw.strip() for kw in text.split(',') if kw.strip()]\n",
    "                return keywords\n",
    "            else:\n",
    "                print(f\"LLM response structure unexpected: {result}\")\n",
    "                time.sleep(2 ** retries) # Exponential backoff\n",
    "                retries += 1\n",
    "                continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API call failed (retry {retries+1}/{max_retries}): {e}\")\n",
    "            time.sleep(2 ** retries) # Exponential backoff\n",
    "            retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            time.sleep(2 ** retries) # Exponential backoff\n",
    "            retries += 1\n",
    "\n",
    "    print(\"Failed to generate keywords after multiple retries.\")\n",
    "    # Fallback to a hardcoded list if API fails after all retries\n",
    "    return [\n",
    "        \"allbirds shoes\", \"rothys shoes\", \"sustainable sneakers\",\n",
    "        \"wool runners\", \"tree dashers\", \"best comfortable travel shoes\",\n",
    "        \"allbirds review\", \"rothys flats\", \"allbirds vs rothys\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def llm_group_keywords(keywords_data, brand_name, competitor_name):\n",
    "    \"\"\"\n",
    "    Simulates an LLM grouping keywords into ad groups based on intent.\n",
    "    \"\"\"\n",
    "    ad_groups = {\n",
    "        \"Brand Terms\": [],\n",
    "        \"Product/Service Category\": [],\n",
    "        \"Competitor Terms\": [],\n",
    "        \"Long-Tail / Informational\": [],\n",
    "        \"Location-Based Queries\": []\n",
    "    }\n",
    "\n",
    "    brand_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        brand_name.replace('.', '\\\\.?'), 'allbirds', 'all birds', 'wool runners', 'tree dashers', 'cubehq', 'cube ai'\n",
    "    ]) + r')\\b'\n",
    "    competitor_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        competitor_name.replace('.', '\\\\.?'), 'rothys', 'rothys shoes', 'reputation.com', 'birdeye'\n",
    "    ]) + r')\\b'\n",
    "    \n",
    "    brand_pattern = re.compile(brand_keywords_regex, re.IGNORECASE)\n",
    "    competitor_pattern = re.compile(competitor_keywords_regex, re.IGNORECASE)\n",
    "\n",
    "    for item in keywords_data:\n",
    "        kw = item['keyword'].lower()\n",
    "        \n",
    "        is_brand_term = bool(brand_pattern.search(kw))\n",
    "        is_competitor_term = bool(competitor_pattern.search(kw))\n",
    "\n",
    "        if is_brand_term and not is_competitor_term:\n",
    "            ad_groups[\"Brand Terms\"].append(item)\n",
    "        elif is_competitor_term:\n",
    "            ad_groups[\"Competitor Terms\"].append(item)\n",
    "        elif \"shoes\" in kw or \"sneakers\" in kw or \"runners\" in kw or \"flats\" in kw or \\\n",
    "             \"marketing platform\" in kw or \"seo\" in kw or \"ads optimization\" in kw or \"reputation management\" in kw:\n",
    "            ad_groups[\"Product/Service Category\"].append(item)\n",
    "        elif \"new york\" in kw or \"los angeles\" in kw or \"london\" in kw or \"berlin\" in kw or \"sydney\" in kw or \\\n",
    "             \"san ramon\" in kw or \"chicago\" in kw or \"scottsdale\" in kw or \"lehi\" in kw or \\\n",
    "             \"liverpool\" in kw or \"munich\" in kw or \"mannheim\" in kw or \"hyderabad\" in kw:\n",
    "            ad_groups[\"Location-Based Queries\"].append(item)\n",
    "        else:\n",
    "            ad_groups[\"Long-Tail / Informational\"].append(item)\n",
    "            \n",
    "    # Assign match types based on ad group\n",
    "    for group in ad_groups:\n",
    "        for item in ad_groups[group]:\n",
    "            if group == \"Brand Terms\":\n",
    "                item['suggested_match_type'] = \"Exact\"\n",
    "            elif group == \"Competitor Terms\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            elif group == \"Product/Service Category\" or group == \"Location-Based Queries\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            else:\n",
    "                item['suggested_match_type'] = \"Broad\"\n",
    "\n",
    "    return ad_groups\n",
    "\n",
    "\n",
    "# --- Web Scraping Function ---\n",
    "def get_website_content(url):\n",
    "    \"\"\"Fetches and scrapes text content from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return \" \".join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- Keyword Planner Data Simulation ---\n",
    "def simulate_keyword_planner_data(keywords):\n",
    "    \"\"\"\n",
    "    Simulates fetching data from a keyword planner API alternative.\n",
    "    Generates random but realistic metrics.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for kw in keywords:\n",
    "        if len(kw.split()) < 3:\n",
    "            avg_monthly_searches = random.randint(1000, 100000)\n",
    "            low_bid = round(random.uniform(0.5, 3.0), 2)\n",
    "            high_bid = round(random.uniform(3.5, 10.0), 2)\n",
    "            competition = \"High\" if random.random() > 0.5 else \"Medium\"\n",
    "        else:\n",
    "            avg_monthly_searches = random.randint(50, 5000)\n",
    "            low_bid = round(random.uniform(0.2, 1.5), 2)\n",
    "            high_bid = round(random.uniform(1.8, 5.0), 2)\n",
    "            competition = \"Medium\" if random.random() > 0.3 else \"Low\"\n",
    "        \n",
    "        data.append({\n",
    "            \"keyword\": kw,\n",
    "            \"avg_monthly_searches\": avg_monthly_searches,\n",
    "            \"top_of_page_bid_low\": low_bid,\n",
    "            \"top_of_page_bid_high\": high_bid,\n",
    "            \"competition\": competition\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- Functions for Deliverables #2 and #3 Output ---\n",
    "\n",
    "def calculate_shopping_bids(shopping_budget, conversion_rate, keywords_data, is_product_based):\n",
    "    \"\"\"\n",
    "    Calculates and suggests CPC bids for a Manual Shopping Campaign.\n",
    "    \"\"\"\n",
    "    if not is_product_based or shopping_budget == 0:\n",
    "        return \"\\n## Deliverable #3: Suggested CPC Bids for Manual Shopping Campaign\\n\\nNot Applicable for this service-based brand or zero budget.\\n\"\n",
    "\n",
    "    average_order_value = 150 # Hypothetical AOV for a product like Allbirds\n",
    "    target_cpa = average_order_value * 0.20 # 20% of AOV\n",
    "    target_cpc = target_cpa * (conversion_rate / 100) # Convert % to decimal\n",
    "\n",
    "    bid_suggestions = f\"\"\"\n",
    "## Deliverable #3: Suggested CPC Bids for Manual Shopping Campaign\n",
    "\n",
    "### Methodology & Calculations:\n",
    "- **Simulated Average Order Value (AOV):** ${average_order_value}\n",
    "- **Target CPA (20% of AOV):** ${target_cpa:.2f}\n",
    "- **Target Conversion Rate:** {conversion_rate}%\n",
    "- **Calculated Target CPC:** ${target_cpc:.2f} (This is our maximum profitable bid per click)\n",
    "\n",
    "### Suggested CPC Bid Strategy for Manual Shopping Campaign:\n",
    "\"\"\"\n",
    "\n",
    "    # Filter for relevant shopping keywords (e.g., product-focused)\n",
    "    shopping_keywords = [item for item in keywords_data if\n",
    "                         'shoes' in item['keyword'].lower() or\n",
    "                         'sneakers' in item['keyword'].lower() or\n",
    "                         'runners' in item['keyword'].lower() or\n",
    "                         'flats' in item['keyword'].lower()]\n",
    "\n",
    "    if not shopping_keywords:\n",
    "        bid_suggestions += \"\\nNo relevant product keywords found for Shopping campaign based on current data.\"\n",
    "        return bid_suggestions\n",
    "\n",
    "    # Sort by monthly searches for prioritization\n",
    "    shopping_keywords.sort(key=lambda x: x['avg_monthly_searches'], reverse=True)\n",
    "\n",
    "    for kw_data in shopping_keywords[:5]: # Take top 5 for example\n",
    "        suggested_bid = target_cpc * 0.8 # Start with a conservative bid relative to target CPC\n",
    "\n",
    "        if kw_data['competition'] == \"High\" and kw_data['top_of_page_bid_low'] > target_cpc:\n",
    "            suggested_bid = max(target_cpc * 1.2, kw_data['top_of_page_bid_low'] * 0.8) # Bid higher to compete\n",
    "        elif kw_data['competition'] == \"Medium\" and kw_data['top_of_page_bid_low'] > target_cpc:\n",
    "            suggested_bid = target_cpc * 1.1 # Slightly above target CPC\n",
    "        \n",
    "        suggested_bid = max(0.1, suggested_bid) # Ensure bid is at least 0.10\n",
    "\n",
    "        bid_suggestions += f\"\"\"\n",
    "- **Product Keyword:** \"{kw_data['keyword']}\"\n",
    "  - **Monthly Searches:** {kw_data['avg_monthly_searches']}\n",
    "  - **Competition:** {kw_data['competition']}\n",
    "  - **Top of Page Bid Range:** ${kw_data['top_of_page_bid_low']} - ${kw_data['top_of_page_bid_high']}\n",
    "  - **Suggested Manual Bid:** ${suggested_bid:.2f}\n",
    "\"\"\"\n",
    "    bid_suggestions += \"\\n*Note: Bids are suggestions and should be continuously optimized based on live campaign performance and ROAS goals.*\\n\"\n",
    "    return bid_suggestions\n",
    "\n",
    "def generate_pmax_themes(ad_groups, brand_name, competitor_name):\n",
    "    \"\"\"\n",
    "    Generates strategic themes for Performance Max Campaign.\n",
    "    \"\"\"\n",
    "    themes = f\"\"\"\n",
    "## Deliverable #2: Search Themes for Performance Max Campaign ({brand_name})\n",
    "\n",
    "These themes are derived from high-performing keyword categories and ad groups, guiding the creation of asset groups for optimal PMax campaign performance.\n",
    "\n",
    "### Product/Service Category Themes:\n",
    "\"\"\"\n",
    "\n",
    "    if ad_groups[\"Product/Service Category\"]:\n",
    "        product_keywords = [item['keyword'] for item in ad_groups[\"Product/Service Category\"]]\n",
    "        themes += f\"\"\"\n",
    "- **Core Offerings:** Focus on the primary products/services.\n",
    "    - Examples: \"{product_keywords[0]}\"{f\", \\\"{product_keywords[1]}\\\"\" if len(product_keywords) > 1 else \"\"}\n",
    "- **Specific Product/Service Lines:** Break down into more granular offerings.\n",
    "    - Examples: \"Sustainable Sneakers\", \"AI Marketing Automation\" (generic examples, adapt to actual keywords)\n",
    "\"\"\"\n",
    "    else:\n",
    "        themes += \"\\n- No specific product/service category themes identified based on current keywords.\"\n",
    "\n",
    "    themes += \"\"\"\n",
    "### Use-Case Based Themes:\n",
    "\"\"\"\n",
    "    if ad_groups[\"Long-Tail / Informational\"]:\n",
    "        informational_keywords = [item['keyword'] for item in ad_groups[\"Long-Tail / Informational\"]]\n",
    "        themes += f\"\"\"\n",
    "- **Problem/Solution Focused:** Address specific customer needs.\n",
    "    - Examples: \"{informational_keywords[0]}\"{f\", \\\"{informational_keywords[1]}\\\"\" if len(informational_keywords) > 1 else \"\"}\n",
    "- **Value Proposition:** Highlight key benefits.\n",
    "    - Examples: \"eco-friendly footwear\", \"AI-driven growth\" (generic examples, adapt to actual keywords)\n",
    "\"\"\"\n",
    "    else:\n",
    "        themes += \"\\n- No specific use-case based themes identified based on current keywords.\"\n",
    "\n",
    "    themes += \"\"\"\n",
    "### Competitive Themes:\n",
    "\"\"\"\n",
    "    if ad_groups[\"Competitor Terms\"]:\n",
    "        competitor_keywords = [item['keyword'] for item in ad_groups[\"Competitor Terms\"]]\n",
    "        themes += f\"\"\"\n",
    "- **Direct Competitor Targeting:** Capture users searching for rivals.\n",
    "    - Examples: \"{competitor_keywords[0]}\"{f\", \\\"{competitor_keywords[1]}\\\"\" if len(competitor_keywords) > 1 else \"\"}\n",
    "- **Comparison Queries:** Engage users comparing brands.\n",
    "    - Examples: \"{brand_name} vs {competitor_name}\"\n",
    "\"\"\"\n",
    "    else:\n",
    "        themes += f\"\\n- No specific competitor themes identified based on current keywords.\"\n",
    "\n",
    "    themes += \"\"\"\n",
    "### Location-Based Themes:\n",
    "\"\"\"\n",
    "    if ad_groups[\"Location-Based Queries\"]:\n",
    "        location_keywords = [item['keyword'] for item in ad_groups[\"Location-Based Queries\"]]\n",
    "        themes += f\"\"\"\n",
    "- **Geographic Targeting:** Focus on specific service areas or store locations.\n",
    "    - Examples: \"{location_keywords[0]}\"{f\", \\\"{location_keywords[1]}\\\"\" if len(location_keywords) > 1 else \"\"}\n",
    "\"\"\"\n",
    "    else:\n",
    "        themes += \"\\n- No specific location-based themes identified based on current keywords.\"\n",
    "\n",
    "    return themes\n",
    "\n",
    "\n",
    "# --- Main Logic ---\n",
    "async def main():\n",
    "    # Load inputs from config.yaml\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    brand_url = config['brand_website']\n",
    "    competitor_url = config['competitor_website']\n",
    "    service_locations = config['service_locations']\n",
    "    shopping_budget = config['ad_budgets']['shopping_ads']\n",
    "\n",
    "    brand_name = brand_url.replace('https://www.', '').split('.')[0]\n",
    "    competitor_name = competitor_url.replace('https://www.', '').split('.')[0]\n",
    "\n",
    "    # Determine if the brand is product-based for Shopping Ads applicability\n",
    "    is_product_based = (shopping_budget > 0) # Simple heuristic: if shopping budget > 0, assume product-based\n",
    "\n",
    "    print(\"--- Step 1: Collecting Inputs ---\")\n",
    "    print(f\"Brand URL: {brand_url}\")\n",
    "    print(f\"Competitor URL: {competitor_url}\")\n",
    "    print(f\"Service Locations: {', '.join(service_locations)}\")\n",
    "    print(f\"Shopping Budget: ${shopping_budget}, Search Budget: ${config['ad_budgets']['search_ads']}, PMax Budget: ${config['ad_budgets']['pmax_ads']}\\n\")\n",
    "\n",
    "    print(\"--- Step 2: Scraping Websites for Keyword Discovery ---\")\n",
    "    brand_content = get_website_content(brand_url)\n",
    "    competitor_content = get_website_content(competitor_url)\n",
    "    print(\"Scraping complete. Generating initial keyword ideas using Gemini API...\\n\")\n",
    "\n",
    "    # Use LLM to generate initial keywords\n",
    "    master_keyword_list = await llm_generate_keywords(brand_content, competitor_content, service_locations)\n",
    "    \n",
    "    print(\"--- Step 3: Simulating Keyword Planner Data ---\")\n",
    "    keyword_df = simulate_keyword_planner_data(master_keyword_list)\n",
    "    # Convert DataFrame to list of dicts for easier manipulation in grouping/output functions\n",
    "    keywords_with_data_list = keyword_df.to_dict('records')\n",
    "    print(f\"Total keywords found (before filtering): {len(keywords_with_data_list)}\\n\")\n",
    "\n",
    "    print(\"--- Step 4: Filtering Keywords (Search Volume > 500) ---\")\n",
    "    filtered_keywords_list = [kw for kw in keywords_with_data_list if kw['avg_monthly_searches'] >= 500]\n",
    "    print(f\"Keywords after filtering: {len(filtered_keywords_list)}\\n\")\n",
    "\n",
    "    print(\"--- Step 5: Grouping Keywords into Ad Groups ---\")\n",
    "    final_ad_groups = llm_group_keywords(filtered_keywords_list, brand_name, competitor_name)\n",
    "\n",
    "    # --- Generate Combined Output File ---\n",
    "    output_filename = \"sem_plan_full_output.txt\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        # Deliverable #1 Content\n",
    "        f.write(f\"## Deliverable #1: Keyword List Grouped by Ad Groups ({brand_name})\\n\\n\")\n",
    "        f.write(\"Based on brand website content, competitor insights, and simulated keyword data with specific location targeting.\\n\\n\")\n",
    "        \n",
    "        for ad_group_name, keywords_in_group in final_ad_groups.items():\n",
    "            if keywords_in_group:\n",
    "                f.write(f\"### Ad Group: {ad_group_name}\\n\")\n",
    "                f.write(\"--------------------------------\\n\")\n",
    "                \n",
    "                for kw_data in keywords_in_group:\n",
    "                    f.write(\n",
    "                        f\" - Keyword: {kw_data['keyword']}\\n\"\n",
    "                        f\"   - Suggested Match Type: {kw_data['suggested_match_type']}\\n\"\n",
    "                        f\"   - Suggested CPC Range: ${kw_data['top_of_page_bid_low']} - ${kw_data['top_of_page_bid_high']}\\n\"\n",
    "                        f\"   - Monthly Searches: {kw_data['avg_monthly_searches']}\\n\"\n",
    "                        f\"   - Competition: {kw_data['competition']}\\n\"\n",
    "                        f\"\\n\"\n",
    "                    )\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        # Deliverable #2 Content\n",
    "        pmax_output = generate_pmax_themes(final_ad_groups, brand_name, competitor_name)\n",
    "        f.write(pmax_output)\n",
    "        f.write(\"\\n\\n\") # Add extra newlines for separation\n",
    "\n",
    "        # Deliverable #3 Content\n",
    "        shopping_bids_output = calculate_shopping_bids(shopping_budget, 2, filtered_keywords_list, is_product_based)\n",
    "        f.write(shopping_bids_output)\n",
    "        f.write(\"\\n\") # Add final newline\n",
    "\n",
    "    print(f\"All deliverables successfully generated and saved to '{output_filename}'\")\n",
    "\n",
    "# This is for running in Jupyter Notebook\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cd9f12-488c-4541-b2e0-5add98f1aa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: streamlit\n"
     ]
    }
   ],
   "source": [
    "pip show streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69033525-3aa7-4489-8e90-134a5369987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.3.2)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (25.0)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.32.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (4.14.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.24.0)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.9/9.9 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.9 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/9.9 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/9.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 7.2 MB/s  0:00:01\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 3.2 MB/s  0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/7.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.5/7.0 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/7.0 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 7.0 MB/s  0:00:00\n",
      "Downloading protobuf-6.32.0-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.9 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.5/6.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/6.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 7.0 MB/s  0:00:00\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading narwhals-2.1.2-py3-none-any.whl (392 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/26.1 MB 7.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.1/26.1 MB 7.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/26.1 MB 7.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.3/26.1 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.9/26.1 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.7/26.1 MB 7.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.5/26.1 MB 6.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 12.1/26.1 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.1/26.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.7/26.1 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 16.3/26.1 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.6/26.1 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.9/26.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.9/26.1 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.0/26.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.3/26.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.6/26.1 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/26.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 6.4 MB/s  0:00:04\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, pyarrow, protobuf, pillow, narwhals, click, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\n",
      "   ----------------------------------------  0/16 [watchdog]\n",
      "   ----------------------------------------  0/16 [watchdog]\n",
      "   ----------------------------------------  0/16 [watchdog]\n",
      "   ----------------------------------------  0/16 [watchdog]\n",
      "   ----------------------------------------  0/16 [watchdog]\n",
      "   -- -------------------------------------  1/16 [toml]\n",
      "   ----- ----------------------------------  2/16 [tenacity]\n",
      "   ----- ----------------------------------  2/16 [tenacity]\n",
      "   ------- --------------------------------  3/16 [smmap]\n",
      "   ------- --------------------------------  3/16 [smmap]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ---------- -----------------------------  4/16 [pyarrow]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   ------------ ---------------------------  5/16 [protobuf]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   --------------- ------------------------  6/16 [pillow]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   ----------------- ----------------------  7/16 [narwhals]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   ---------------------- -----------------  9/16 [cachetools]\n",
      "   ------------------------- -------------- 10/16 [blinker]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   --------------------------- ------------ 11/16 [pydeck]\n",
      "   ------------------------------ --------- 12/16 [gitdb]\n",
      "   ------------------------------ --------- 12/16 [gitdb]\n",
      "   ------------------------------ --------- 12/16 [gitdb]\n",
      "   ------------------------------ --------- 12/16 [gitdb]\n",
      "   ------------------------------ --------- 12/16 [gitdb]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   -------------------------------- ------- 13/16 [gitpython]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ----------------------------------- ---- 14/16 [altair]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ------------------------------------- -- 15/16 [streamlit]\n",
      "   ---------------------------------------- 16/16 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.1.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 narwhals-2.1.2 pillow-11.3.0 protobuf-6.32.0 pyarrow-21.0.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.48.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit requests beautifulsoup4 pandas pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4745d63-ad5c-4262-a02c-ff960379c584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit version: 1.48.1\n",
      "Requests version: 2.32.3\n",
      "BeautifulSoup4 (bs4) version: 4.13.4\n",
      "Pandas version: 2.3.1\n",
      "PyYAML (yaml) version: 6.0.2\n"
     ]
    }
   ],
   "source": [
    "import streamlit\n",
    "import requests\n",
    "import bs4 # BeautifulSoup is part of the bs4 package\n",
    "import pandas\n",
    "import yaml # PyYAML is imported as yaml\n",
    "\n",
    "print(f\"Streamlit version: {streamlit.__version__}\")\n",
    "print(f\"Requests version: {requests.__version__}\")\n",
    "print(f\"BeautifulSoup4 (bs4) version: {bs4.__version__}\")\n",
    "print(f\"Pandas version: {pandas.__version__}\")\n",
    "print(f\"PyYAML (yaml) version: {yaml.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db75c7a7-fea7-4628-9f30-523c56c25688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omen\\sem assigment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e72113-19d1-45db-bb51-ac712515f796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
