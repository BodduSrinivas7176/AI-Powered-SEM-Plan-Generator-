{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3536c2ff-8aff-4b43-b894-2b884ed6ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Collecting Inputs ---\n",
      "Brand URL: https://www.allbirds.com\n",
      "Competitor URL: https://www.rothys.com\n",
      "Service Locations: New York, NY, Los Angeles, CA, London, UK, Berlin, Germany, Sydney, Australia\n",
      "\n",
      "--- Step 2: Scraping Websites for Keyword Discovery ---\n",
      "Scraping complete. Generating initial keyword ideas using Gemini API...\n",
      "\n",
      "--- Step 3: Simulating Keyword Planner Data ---\n",
      "Total keywords found (before filtering): 20\n",
      "\n",
      "--- Step 4: Filtering Keywords (Search Volume > 500) ---\n",
      "Keywords after filtering: 19\n",
      "\n",
      "--- Step 5: Grouping Keywords into Ad Groups ---\n",
      "Deliverable successfully generated and saved to 'sem_deliverable_1_output.txt'\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "\n",
    "# --- Gemini API Configuration (Leave API key empty, Canvas will provide) ---\n",
    "API_KEY = \"AIzaSyCKNKW9HKVwVjQRDeT0lbSUz8Jh-FIeE9M\"\n",
    "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent\"\n",
    "\n",
    "# --- LLM Simulation Functions ---\n",
    "async def llm_generate_keywords(brand_content, competitor_content, locations):\n",
    "    \"\"\"\n",
    "    Generates keywords using the Gemini API based on website content.\n",
    "    Includes exponential backoff for API calls.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following content from a brand's website and its competitor.\n",
    "    Identify 10-15 highly relevant, high-intent seed keywords that a potential customer\n",
    "    would use to search for these products/services. Include brand terms, competitor terms,\n",
    "    and general category terms. Also, consider adding location-specific keywords for these areas: {', '.join(locations)}.\n",
    "    Provide the keywords as a comma-separated list.\n",
    "\n",
    "    Brand Content (from {brand_content[:100]}...):\n",
    "    {brand_content[:1000]}\n",
    "\n",
    "    Competitor Content (from {competitor_content[:100]}...):\n",
    "    {competitor_content[:1000]}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_history = []\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n",
    "    payload = {\"contents\": chat_history}\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    full_api_url = f\"{API_URL}?key={API_KEY}\"\n",
    "\n",
    "    retries = 0\n",
    "    max_retries = 5\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(full_api_url, headers=headers, data=json.dumps(payload))\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "\n",
    "            if result.get(\"candidates\") and result[\"candidates\"][0].get(\"content\") and result[\"candidates\"][0][\"content\"].get(\"parts\"):\n",
    "                text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "                keywords = [kw.strip() for kw in text.split(',') if kw.strip()]\n",
    "                return keywords\n",
    "            else:\n",
    "                print(f\"LLM response structure unexpected: {result}\")\n",
    "                time.sleep(2 ** retries)\n",
    "                retries += 1\n",
    "                continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API call failed (retry {retries+1}/{max_retries}): {e}\")\n",
    "            time.sleep(2 ** retries)\n",
    "            retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            time.sleep(2 ** retries)\n",
    "            retries += 1\n",
    "\n",
    "    print(\"Failed to generate keywords after multiple retries.\")\n",
    "    return [\n",
    "        \"allbirds shoes\", \"rothys shoes\", \"sustainable sneakers\",\n",
    "        \"wool runners\", \"tree dashers\", \"best comfortable travel shoes\",\n",
    "        \"allbirds review\", \"rothys flats\", \"allbirds vs rothys\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def llm_group_keywords(keywords_data, brand_name, competitor_name):\n",
    "    \"\"\"\n",
    "    Simulates an LLM grouping keywords into ad groups based on intent.\n",
    "    \"\"\"\n",
    "    ad_groups = {\n",
    "        \"Brand Terms\": [],\n",
    "        \"Product/Service Category\": [],\n",
    "        \"Competitor Terms\": [],\n",
    "        \"Long-Tail / Informational\": [],\n",
    "        \"Location-Based Queries\": []\n",
    "    }\n",
    "\n",
    "    brand_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        brand_name.replace('.', '\\\\.?'), 'allbirds', 'all birds', 'wool runners', 'tree dashers'\n",
    "    ]) + r')\\b'\n",
    "    competitor_keywords_regex = r'\\b(?:' + '|'.join([\n",
    "        competitor_name.replace('.', '\\\\.?'), 'rothys', 'rothys shoes', 'reputation.com'\n",
    "    ]) + r')\\b'\n",
    "    \n",
    "    brand_pattern = re.compile(brand_keywords_regex, re.IGNORECASE)\n",
    "    competitor_pattern = re.compile(competitor_keywords_regex, re.IGNORECASE)\n",
    "\n",
    "    for item in keywords_data:\n",
    "        kw = item['keyword'].lower()\n",
    "        \n",
    "        is_brand_term = bool(brand_pattern.search(kw))\n",
    "        is_competitor_term = bool(competitor_pattern.search(kw))\n",
    "\n",
    "        if is_brand_term and not is_competitor_term:\n",
    "            ad_groups[\"Brand Terms\"].append(item)\n",
    "        elif is_competitor_term:\n",
    "            ad_groups[\"Competitor Terms\"].append(item)\n",
    "        elif \"shoes\" in kw or \"sneakers\" in kw or \"runners\" in kw or \"flats\" in kw or \\\n",
    "             \"marketing platform\" in kw or \"seo\" in kw or \"ads optimization\" in kw or \"reputation management\" in kw:\n",
    "            ad_groups[\"Product/Service Category\"].append(item)\n",
    "        elif \"new york\" in kw or \"los angeles\" in kw or \"london\" in kw or \"berlin\" in kw or \"sydney\" in kw or \\\n",
    "             \"san ramon\" in kw or \"chicago\" in kw or \"scottsdale\" in kw or \"lehi\" in kw or \\\n",
    "             \"liverpool\" in kw or \"munich\" in kw or \"mannheim\" in kw or \"hyderabad\" in kw:\n",
    "            ad_groups[\"Location-Based Queries\"].append(item)\n",
    "        else:\n",
    "            ad_groups[\"Long-Tail / Informational\"].append(item)\n",
    "            \n",
    "    for group in ad_groups:\n",
    "        for item in ad_groups[group]:\n",
    "            if group == \"Brand Terms\":\n",
    "                item['suggested_match_type'] = \"Exact\"\n",
    "            elif group == \"Competitor Terms\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            elif group == \"Product/Service Category\" or group == \"Location-Based Queries\":\n",
    "                item['suggested_match_type'] = \"Phrase\"\n",
    "            else:\n",
    "                item['suggested_match_type'] = \"Broad\"\n",
    "\n",
    "    return ad_groups\n",
    "\n",
    "\n",
    "# --- Web Scraping Function ---\n",
    "def get_website_content(url):\n",
    "    \"\"\"Fetches and scrapes text content from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return \" \".join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- Keyword Planner Data Simulation ---\n",
    "def simulate_keyword_planner_data(keywords):\n",
    "    \"\"\"\n",
    "    Simulates fetching data from a keyword planner API alternative.\n",
    "    Generates random but realistic metrics.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for kw in keywords:\n",
    "        if len(kw.split()) < 3:\n",
    "            avg_monthly_searches = random.randint(1000, 100000)\n",
    "            low_bid = round(random.uniform(0.5, 3.0), 2)\n",
    "            high_bid = round(random.uniform(3.5, 10.0), 2)\n",
    "            competition = \"High\" if random.random() > 0.5 else \"Medium\"\n",
    "        else:\n",
    "            avg_monthly_searches = random.randint(50, 5000)\n",
    "            low_bid = round(random.uniform(0.2, 1.5), 2)\n",
    "            high_bid = round(random.uniform(1.8, 5.0), 2)\n",
    "            competition = \"Medium\" if random.random() > 0.3 else \"Low\"\n",
    "        \n",
    "        data.append({\n",
    "            \"keyword\": kw,\n",
    "            \"avg_monthly_searches\": avg_monthly_searches,\n",
    "            \"top_of_page_bid_low\": low_bid,\n",
    "            \"top_of_page_bid_high\": high_bid,\n",
    "            \"competition\": competition\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- Main Logic ---\n",
    "async def main():\n",
    "    # Load inputs from config.yaml\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    brand_url = config['brand_website']\n",
    "    competitor_url = config['competitor_website']\n",
    "    service_locations = config['service_locations']\n",
    "    brand_name = brand_url.replace('https://www.', '').split('.')[0]\n",
    "    competitor_name = competitor_url.replace('https://www.', '').split('.')[0]\n",
    "\n",
    "\n",
    "    print(\"--- Step 1: Collecting Inputs ---\")\n",
    "    print(f\"Brand URL: {brand_url}\")\n",
    "    print(f\"Competitor URL: {competitor_url}\")\n",
    "    print(f\"Service Locations: {', '.join(service_locations)}\\n\")\n",
    "\n",
    "    print(\"--- Step 2: Scraping Websites for Keyword Discovery ---\")\n",
    "    brand_content = get_website_content(brand_url)\n",
    "    competitor_content = get_website_content(competitor_url)\n",
    "    print(\"Scraping complete. Generating initial keyword ideas using Gemini API...\\n\")\n",
    "\n",
    "    # Use LLM to generate initial keywords\n",
    "    master_keyword_list = await llm_generate_keywords(brand_content, competitor_content, service_locations)\n",
    "    \n",
    "    print(\"--- Step 3: Simulating Keyword Planner Data ---\")\n",
    "    keyword_df = simulate_keyword_planner_data(master_keyword_list)\n",
    "    print(f\"Total keywords found (before filtering): {len(keyword_df)}\\n\")\n",
    "\n",
    "    print(\"--- Step 4: Filtering Keywords (Search Volume > 500) ---\")\n",
    "    filtered_df = keyword_df[keyword_df['avg_monthly_searches'] >= 500]\n",
    "    print(f\"Keywords after filtering: {len(filtered_df)}\\n\")\n",
    "\n",
    "    print(\"--- Step 5: Grouping Keywords into Ad Groups ---\")\n",
    "    final_keywords_dict = llm_group_keywords(filtered_df.to_dict('records'), brand_name, competitor_name)\n",
    "\n",
    "    output_filename = \"sem_deliverable_1_output.txt\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(f\"## Deliverable #1: Keyword List Grouped by Ad Groups ({brand_name})\\n\\n\")\n",
    "        f.write(\"Based on brand website content, competitor insights, and simulated keyword data with specific location targeting.\\n\\n\")\n",
    "        \n",
    "        for ad_group, keywords in final_keywords_dict.items():\n",
    "            if keywords:\n",
    "                f.write(f\"### Ad Group: {ad_group}\\n\")\n",
    "                f.write(\"--------------------------------\\n\")\n",
    "                \n",
    "                for kw_data in keywords:\n",
    "                    f.write(\n",
    "                        f\" - Keyword: {kw_data['keyword']}\\n\"\n",
    "                        f\"   - Suggested Match Type: {kw_data['suggested_match_type']}\\n\"\n",
    "                        f\"   - Suggested CPC Range: ${kw_data['top_of_page_bid_low']} - ${kw_data['top_of_page_bid_high']}\\n\"\n",
    "                        f\"   - Monthly Searches: {kw_data['avg_monthly_searches']}\\n\"\n",
    "                        f\"   - Competition: {kw_data['competition']}\\n\"\n",
    "                        f\"\\n\"\n",
    "                    )\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "    print(f\"Deliverable successfully generated and saved to '{output_filename}'\")\n",
    "\n",
    "# At the very end of the cell:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85501489-b751-419e-87d2-cc39ca8043d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
